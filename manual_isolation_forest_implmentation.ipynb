{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forests\n",
    "\n",
    "> Objective\n",
    "> * Implement the Isolation Forest (iForest) anomaly detection algorithm\n",
    "\n",
    "Original [\"Isolation Forest\"](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf) paper\n",
    "\n",
    "### Importance to project\n",
    "\n",
    "Compared to all previous methods, the isolation forest method provides a very different approach to finding anomalies. Instead of explicitly relying on statistics such as mean or covariance, or probabilities such as the cumulative distribution function, it is based upon the concepts of decision trees and ensemble methods widely used in the supervised machine learning literature. In this project, I will implement the Isolation forest anomaly detection method.\n",
    "\n",
    "## Isolation forest in a nutshell\n",
    "\n",
    "Isolation forests exploit the heuristic that anomalies tend to be isolated points. That is, points that are separated from the rest either because they are characterized by extreme values along some feature dimension or because they are in the region of rather low density. Instead of modeling these distance or density metrics like, for example, PCA or ECOD, the method seeks to directly isolate anomalous points.\n",
    "\n",
    "The method isolates points by repeatedly partitioning the data. The partition criteria are randomly built at every iteration; first, a feature for analysis is randomly selected, then for that feature, its smaller and larger values are found, and then a `split value` is generated by drawing a uniform random number between the feature’s min and max value. All observations whose analyzed features have a value less than the `split value` are gathered into one group; the reminder observations are gathered in a separate group. In turn, each of the subgroups will be partitioned in a similar fashion. At each step, the information of what feature and split value was used is recorded in a node together with the children nodes that will contain similar information for each of the subgroups. The process is carried out until the isolation tree reaches a maximum height of $\\log_2(\\psi)$ where $\\psi$ is the number of sub-samples used to build the tree and is a parameter of the algorithm. When the maximum tree height is reached, the leaf nodes only store the information of how many samples were allocated to it.\n",
    "\n",
    "For example, in the figure below, the data is first split according to the values of feature 5, it is found that only a single observation has a feature 5 value larger than the split value of -3, so that observation is directly isolated. The remaining observations are further split according to the value of feature 3 and at later stages features 1 and 5. At that point the maximum height of the tree is reached and the leaf nodes only contain information about the number of samples allocated to them.\n",
    "\n",
    "The intuition of the algorithm, is that anomalous endpoints tend to be closer to the root of the tree. This notion of closeness is formalized by the `path length` from the root to the isolating node i.e. the number of steps taken to reach the node from the root of the tree. The shorter the path length the more anomalous an endpoint is. Notice that any sample can always be scored even if it was not used to build the tree, it is enough to apply the partition criteria at every node until a leaf node is reached.\n",
    "\n",
    "Using a single tree to score all observations would lead to results with high variance, this is why the algorithm builds a predefined number of trees and generates the anomaly score by averaging the path length over all the trees. The final anomaly anomaly score is given by the formula.\n",
    "\n",
    "$$S(x) = 2^{-E(x)/C(n)}$$\n",
    "\n",
    "$E(x)$ is the actual path length average of sample $x$ over all trees and $C(n)$ is a normalization factor. Technically, $C(n)$ calculates the average path length of an unsuccessful search on a binary search tree with $n$ nodes. The reason for using it is that it allows to compare results that might have been generated by trees of different shapes.\n",
    "\n",
    "The method has two parameters, subsample size and the number of trees, in the unsupervised setting this is usually a drawback because of the difficulty of tuning parameters. In the case of isolation forests, it turns out that the algorithm is rather insensitive to the values of these parameter and that values around of 100 to 1000 trees and sub sample sizes of 256 to 1000 tend to deliver similar results across multiple data sets.\n",
    "\n",
    "There are many technical details to each of the steps needed to implement the isolation fores method, but don’t worry you will work on them one step at a time.\n",
    "  \n",
    "<br>   \n",
    "<div align=\"center\">\n",
    "<img src=\"https://d16rtcb5cr0vb4.cloudfront.net/C1615+Advanced+Multidimensional+Anomaly+Detection+with+Isolation+Forests%2FResources%2FImages%2FP4_ISOforest_for+approval_V1.png?Expires=1691070962&Signature=E8OGgSgeWGtJRARbW372UYICHgZFHbHJObZDMor6Dzdt6z7cLe6LDAUo3kVLI7aE11tGN6TXy87DFCyv9u6h2tuiBq5C5WxrdvrHrLcq3vahj7XP3GZ02wtAk0mpCm80EX4U6nh8qIUKojkfYp6msaEyUIVPRhIUUVTWzNb4S6lS0sceu080U9PLo1lV~ra8mx3aU8DYCex4SSe2uYgX2SXVabAU~cwtoEvC0h2qvNTk0vsR~iZodQguNDi3BD8ZSHrsHaoRGlo0uZ9DZ6PsPwTGAU04~P0MBFSLxblrONx5o7LJ2ucmaY~-n1TpHaMgG6p3tur4Cy7E7oUG1mdoEA__&Key-Pair-Id=APKAIHLKH2FX732Z3HGA\" alt=\"drawing\" width=\"35%\"/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "The isolation forest method is an ensemble method. This means it combines the predictions made by multiple simple components into one improved result. In this case, the simple components are isolation trees. Technically, an isolation tree is a binary tree where each internal node contains a feature index, a split value, and two children nodes. External or leaf nodes contain only the number of observations that were assigned to it at training time.\n",
    "\n",
    "The approach to implement the Isolation forest is going to be:\n",
    "\n",
    "* A) Define the tree data structures  \n",
    "* B) Implement the tree-building function  \n",
    "* C) Implement the forest building function  \n",
    "* D) Implement the scoring function  \n",
    "* E) Assemble all the pieces into the `IsoForestScorer` class  \n",
    "  \n",
    "In the starter file, you will find the stubs for the relevant functions that will be implemented. Additionally, you will find various tests that should help you while developing.  \n",
    "To run them, use `pytest pytest -v tests/milestone4`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Tree data structures\n",
    "The isolation trees data structures are:\n",
    "\n",
    "```python\n",
    "class Node:\n",
    "    pass\n",
    "\n",
    "@dataclass()\n",
    "class ExNode(Node):\n",
    "    size: int\n",
    "\n",
    "@dataclass()\n",
    "class InNode(Node):\n",
    "    left: Node\n",
    "    right: Node\n",
    "    splitAtt: int\n",
    "    splitValue: float\n",
    "```\n",
    "\n",
    "The `Node` class has two sub-classes:\n",
    "\n",
    "`ExNode`: That describes the outermost nodes when no further partitions are made  \n",
    "`INode`: That describes the inner nodes of the tree.\n",
    "\n",
    "## B) Tree building\n",
    "The algorithm for building a single tree is\n",
    "\n",
    "`iTree(X,e,l)`\n",
    "\n",
    "**Inputs:** observations $X$ ($n$ observations each with $d$ features), current tree height $e$ and max tree hight $l$.\n",
    "\n",
    "**Output:** an isolation tree\n",
    "\n",
    "`if` $e≥l$ `or` $∣ X ∣≤1$ `then`\n",
    "\n",
    "`return ExternalNode(size = |X|)`\n",
    "\n",
    "`else`\n",
    "\n",
    "split_attribute <- randomly select the index of a feature\n",
    "\n",
    "split_value <- uniform random number between the min and max value of all values of feature `split_attribute` in $X$\n",
    "\n",
    "left_data <- all samples in $X$ such that their `split_attribute` feature is smaller than `split_value`\n",
    "\n",
    "right_data <- all samples in $X$ such that their `split_attribute` feature is larger or equal than `split_value`\n",
    "\n",
    "left_node = `iTree(left_data,e+1,l)`\n",
    "\n",
    "right_node = `iTree(right_data,e+1,l)`\n",
    "\n",
    "`return InternalNode(left_node,right_node,split_attribute,split_value)`\n",
    "\n",
    "It can be implemented in the following way:\n",
    "\n",
    "1. Implement the `split_data(data, feature_index, split_point)` function that takes a set of observations, a feature index and a split point and returns left_data and right_data\n",
    "\n",
    "2. Implement the `calculate_split_point(data, feature_index, rng)` function that takes a set of observations, a feature index and a random number generator. It returns a random number uniformly sampled between the min and max values of the feature corresponding to feature_index in all of data\n",
    "\n",
    "3. Implement the `build_tree(data, current_height, max_height, rng)` function, that corresponds to `iTree(X,e,l)`, using the `split_data` and `calculate_split_point` functions.\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src='img/iTree_algo.png' alt='iTree alogirthm' width=\"30%\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start with utils\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def matlab_to_numpy(path_to_mat_file: str)-> np.ndarray:\n",
    "    \"\"\"Convert MATLAB .mat file to Numpy array\n",
    "\n",
    "    Args:\n",
    "        path_to_mat_file (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: _description_\n",
    "    \"\"\"\n",
    "    data=scipy.io.loadmat(path_to_mat_file)\n",
    "    data=data['X']\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "class Node:\n",
    "    pass\n",
    "\n",
    "@dataclass()\n",
    "class ExNode(Node):\n",
    "    size: int\n",
    "\n",
    "@dataclass()\n",
    "class InNode(Node):\n",
    "    left: Node\n",
    "    right: Node\n",
    "    splitAtt: int\n",
    "    splitValue: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with split_data function\n",
    "\n",
    "\n",
    "def split_data(data: np.ndarray, \n",
    "               feature_index: int, \n",
    "               split_point: int):\n",
    "    \"\"\"Takes a set of observations, a feature index and a \n",
    "    split point and returns left_data and right_data\n",
    "\n",
    "    Args:\n",
    "        data (_type_): _description_ MATLAB matrix composed of X and y dict\n",
    "        feature_index (_type_): _description_\n",
    "        split_point (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    left_data = data[:split_point, feature_index]\n",
    "    right_data = data[split_point:, feature_index]\n",
    "    \n",
    "    return (left_data, right_data)\n",
    "\n",
    "def calculate_split_point(data: dict, \n",
    "                          feature_index: int, \n",
    "                          rng: np.random._generator.Generator) -> int:\n",
    "    \"\"\"Takes a set of observations, a feature index and a random number \n",
    "    generator. It returns a random number uniformly sampled between the \n",
    "    min and max values of the feature corresponding to feature_index in \n",
    "    all of data\n",
    "\n",
    "    Args:\n",
    "        data (dict): _description_\n",
    "        feature_index (int): _description_\n",
    "        rng (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        int: _description_\n",
    "    \"\"\"\n",
    "    min_feat = min(data[:, feature_index])\n",
    "    max_feat = max(data[:, feature_index])\n",
    "    rnd_nbr = rng.integers(min_feat, max_feat)\n",
    "    \n",
    "    return rnd_nbr\n",
    "\n",
    "def build_tree(data: dict, \n",
    "               current_height: int, \n",
    "               max_height: int, \n",
    "               rng: np.random._generator.Generator):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        data (dict): _description_\n",
    "        current_height (int): _description_\n",
    "        max_height (int): _description_\n",
    "        rng (None): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    dim_n, dim_d = np.shape(data)\n",
    "    \n",
    "    if current_height>=max_height or dim_n<=1:\n",
    "     \n",
    "        return ExNode(size=dim_n)\n",
    "    \n",
    "    else:\n",
    "        feature_index = rng.integers(dim_d)\n",
    "        split_point = calculate_split_point(data, feature_index, rng)\n",
    "        left_data, right_data = split_data(data, \n",
    "                                           feature_index, \n",
    "                                           split_point)\n",
    "        left_node = build_tree(left_data, \n",
    "                               current_height + 1, \n",
    "                               max_height, \n",
    "                               rng)\n",
    "        right_node = build_tree(right_data, \n",
    "                                current_height + 1, \n",
    "                                max_height, \n",
    "                                rng)\n",
    "\n",
    "        return InNode(left_node, right_node, feature_index, split_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[268 242 300 371 150 216 323 212 242   0]\n",
      "[426  85 543   0 190 351 150 150 170 162]\n",
      "976\n"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "test_data = matlab_to_numpy('datasets/cover.mat')\n",
    "\n",
    "# 1st function\n",
    "left, right = split_data(test_data, 3, 34)\n",
    "print(left[:10])\n",
    "print(right[:10])\n",
    "\n",
    "# 2nd function\n",
    "rng_test = np.random.default_rng(12345)\n",
    "\n",
    "split_point_test = calculate_split_point(test_data, 3, rng_test)\n",
    "print(split_point_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Forest building\n",
    "An isolation forest is just a collection of isolation trees. The algorithm is:\n",
    "\n",
    "`IForest(X,t,s)`\n",
    "\n",
    "**Inputs:** observations $X$ ($n$ observations each with $d$ features), number of tree $t$ sub sample size $s$  \n",
    "**Output:** Isolation forest, list of isolation trees.\n",
    "\n",
    "`initialize Forest` (list that will contain the trees)\n",
    "\n",
    "max_height <- $ceiling(log_2(s))$\n",
    "\n",
    "`for` i = 1 to t `do` \n",
    "\n",
    "$X'$  <- sample without replacement $s$ samples from $X$\n",
    "\n",
    "iTree <- `iTree(X',0,max_height)`\n",
    "\n",
    "append `iTree` to `Forest`\n",
    "\n",
    "`return Forest`\n",
    "\n",
    "1. Implement `IForest(X,t,s)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IForest(data: dict, t: int, s: int):\n",
    "    rng = np.random.default_rng(12345)\n",
    "    max_height = np.ceil(np.log(s))\n",
    "    forest = []\n",
    "    for _ in range(t):\n",
    "        sampled_data = rng.choice(data, s, replace=False)\n",
    "        tree = build_tree(sampled_data, 0, max_height, rng)\n",
    "        forest.append(tree)\n",
    "\n",
    "    return forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Scoring function\n",
    "To calculate the anomaly score for a given sample, it is required to first calculate the path length of that sample on all trees and second to average and normalize the result.\n",
    "\n",
    "### Path length algorithm:\n",
    "\n",
    "`PathLength(x,T,e)`\n",
    "\n",
    "**Inputs:** $x$ single observation, $T$ an isolation tree, $e$ current path length  \n",
    "**Output:** path length of $x$ `if` $T$ is an external node `then` return $e + c_n(T.size)$ `end if`\n",
    "\n",
    "a <- T.split_attribute\n",
    "\n",
    "x_a <- a-th feature of $x$\n",
    "\n",
    "if x_a < T.split_value then return PathLength(x,T.left,e + 1) else return PathLength(x,T.right,e + 1)\n",
    "\n",
    "$c_n(.)$ is a function that returns the average path length of an unsuccessful search on a binary search tree.\n",
    "\n",
    "### Score sample algorithm:\n",
    "\n",
    "`ScoreSample(x,F,s)`\n",
    "\n",
    "**Inputs:** observation $x$, isolation forest $F$ sub sample size $s$  \n",
    "**Output:** Outlier score of $x$\n",
    "\n",
    "initialize PL (list that will contain the path scores)\n",
    "\n",
    "for each tree T in F path_length <- PathLength(x,T,e) append path_length to PL\n",
    "\n",
    "mean_path_length =mean of PL values\n",
    "\n",
    "$score <- 2^{\\frac{mean \\space path \\space length}{c_n(s)}}$  \n",
    "\n",
    "return score\n",
    "\n",
    "1. Implement the path length algorithm; see the `path_length` function in the starter file\n",
    "2. Implement the score sample algorithm; see the `score_sample` function in the starter file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_n(n):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        n: number of nodes in a binary search tree\n",
    "\n",
    "    Returns\n",
    "        average path length of an unsuccessful search on the try\n",
    "    \"\"\"\n",
    "    if n > 2:\n",
    "        h_n_1 = np.log(n - 1) + np.euler_gamma\n",
    "        return 2 * h_n_1 - (2.0 * (n - 1) / n)\n",
    "    elif n == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def path_length(sample, \n",
    "                tree, \n",
    "                current_path_length):\n",
    "\n",
    "    match tree:\n",
    "        case ExNode(size):\n",
    "            return current_path_length + c_n(size)\n",
    "        case InNode(left, right, splitAtt, splitValue):\n",
    "            if sample[splitAtt] < splitValue:\n",
    "                return path_length(sample, \n",
    "                                   left, \n",
    "                                   current_path_length + 1)\n",
    "            else:\n",
    "                return path_length(sample, \n",
    "                                   right, \n",
    "                                   current_path_length + 1)\n",
    "\n",
    "\n",
    "def score_sample(sample, \n",
    "                 forest, \n",
    "                 num_samples):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sample: single observation i.e instance to be scored\n",
    "        forest: list of trees\n",
    "        num_samples: number of samples used to build the trees in `forest`\n",
    "\n",
    "    Returns\n",
    "        outlier score of `sample`\n",
    "    \"\"\"\n",
    "    path_len = []\n",
    "    for tree in forest:\n",
    "        sample_path_len = path_length(sample, tree, 0)\n",
    "        path_len.append(sample_path_len)\n",
    "\n",
    "    mean_path_length = np.mean(path_len)\n",
    "    exp = mean_path_length / c_n(num_samples)\n",
    "    score = np.float_power(2.0, -exp)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E) IsoForestScorer\n",
    "Bring all the pieces together and implement the `IsoForestScorer` class\n",
    "\n",
    "1. Implement the initialization logic __init__.\n",
    "    * what data should be kept for the score function to work?\n",
    "    \n",
    "2. Implement the score functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsoForestScorer:\n",
    "\n",
    "    def __init__(self, data, num_trees, sub_sample_size, rng) -> None:\n",
    "\n",
    "        self.sub_sample_size = sub_sample_size\n",
    "        self.forest = IForest(data, num_trees, sub_sample_size, rng)\n",
    "\n",
    "    def score(self, samples):\n",
    "\n",
    "        def _scorer(sample):\n",
    "            return score_sample(sample, self.forest, self.sub_sample_size)\n",
    "\n",
    "        return np.apply_along_axis(_scorer, 1, samples)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F) Final check\n",
    "The isolation forest algorithm is certainly not trivial; by this point, you should have a working version of it.\n",
    "\n",
    "1. Run the starter file and verify that your implementation produces the same results as those presented in the original publication and in the implementation provided by scikit learn.\n",
    "\n",
    "**Note:** the\n",
    "\n",
    "* mammography (http://odds.cs.stonybrook.edu/mammography-dataset/)\n",
    "* breast (http://odds.cs.stonybrook.edu/mammography-dataset/)\n",
    "* shuttle (http://odds.cs.stonybrook.edu/shuttle-dataset/)\n",
    "  \n",
    "datasets must be available on the `datasets` folder of the project to run the final check."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
